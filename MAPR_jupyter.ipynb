{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93510ab4",
   "metadata": {},
   "source": [
    "# Exercise project (TKO_3120 Machine Learning and Pattern Recognition, 5 ECTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fadc07",
   "metadata": {},
   "source": [
    "The goal of this project was to find the best performing supervised machine learning model with optimized hyperparameters to efficiently predict the class of a rice grain based on its extracted characteristics.  \n",
    " \n",
    "The data consisted of images of arborio, basmati, ipsala, jasmine and caracadag rice grains against a black background, 500 samples in total. The project follows the findings of a previous study by Cinar et.al. entitled Identification of Rice Varieties Using Machine Learning Algorithms,  and the used samples were randomly picked from a collection of 75 000 images using a seed (50).  \n",
    "\n",
    "For preprocessing, the data was standardized and dimensionality reduction (PCA) was applied for proper visualization of the clusters. Missing values were processed accordingly and the features were extracted based on their morphological, color and shape features. Random Forest, Support Vector Machine and MLP were chosen as the model candidates and the final cross validation was performed as a nested cross-validation which revealed that the model with the best accuracy is the Support Vector Machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c0dc30",
   "metadata": {},
   "source": [
    "Original research article:\n",
    "\n",
    "İ. Çınar and M. Koklu. Identification of rice varieties using machine learning algorithms. Journal of Agricultural Sciences, 28(2):307–325, 2022. doi: 10.15832/ankutbd.862482.\n",
    "\n",
    "https://dergipark.org.tr/en/download/article-file/1513632\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f605da2",
   "metadata": {},
   "source": [
    "## Preparations of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67124354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Importing libraries for file manipulation and image processing\n",
    "import glob, os\n",
    "import cv2 as cv\n",
    "\n",
    "# Importing libraries for statistical analysis\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "# Importing libraries for machine learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV, StratifiedKFold, cross_val_predict\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Importing libraries for miscellaneous functionalities\n",
    "from random import sample, seed\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2221ff2",
   "metadata": {},
   "source": [
    "\n",
    "The rice sample images were imported from the link http://www.muratkoklu.com/datasets/vtdhnd09.php. These were separated into folders by rice species: 'Arborio', 'Basmati', 'Ipsala', 'Jasmine', and 'Karacadag'. Then, 100 random images were sampled from each species, totaling 500 images. The seed(50) method was used for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b02035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "# set the seed for enabling the reproduction with the same sequence\n",
    "seed(50)\n",
    "\n",
    "path = '../data'\n",
    "folders = ['Arborio', 'Basmati', 'Ipsala', 'Jasmine', 'Karacadag']\n",
    "\n",
    "all_images = []\n",
    "subset = []\n",
    "for folder in folders:\n",
    "    path_folder = os.path.join(path, folder)\n",
    "    # all .jpg files from the given folder\n",
    "    files = glob.glob(os.path.join(path_folder, '*.jpg'))\n",
    "    # gather all sampled filenames in subset list\n",
    "    subset.extend(sample(files, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e68177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering the sampled images into a list\n",
    "image_list =[]\n",
    "# Iterating through the subset list and reading the images\n",
    "for image in subset:\n",
    "    image_list.append(cv.imread(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c96a98",
   "metadata": {},
   "source": [
    "basmati (244).jpg is used as a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f11b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the test image\n",
    "test_image = cv.imread( '../data\\\\Basmati\\\\basmati (244).jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7d71fc",
   "metadata": {},
   "source": [
    "The contours of each rice were determined using the findContours function from OpenCV. The contour for the test image was also identified. To visualize these results, both the original test image and its image with the contour were plotted.\n",
    "\n",
    "To avoid modifying the original image when using drawCountours, a copy of the test image was utilized as input for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca4266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for finding contours\n",
    "def find_contours (image):\n",
    "    # Grayscale conversion\n",
    "    img_gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    # Binary thresholding (0 is black, 255 is white. Values greater than 150 = white)\n",
    "    ret, thresh = cv.threshold(img_gray, 127, 255, 0)\n",
    "    # Identify contours, RETR_TREE retrieves all the contours and creates a tree hierarchy (suitable for the grains against a black background), CHAIN_APPROX_NONE stores all contour points\n",
    "    contours, hierarchy = cv.findContours(image=thresh, mode=cv.RETR_TREE, method=cv.CHAIN_APPROX_SIMPLE)\n",
    "    return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31215e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the test image to avoid overwrite\n",
    "test_copy = test_image.copy()\n",
    "\n",
    "# Drawing of the contours to the copy, ContourIdx=-1 to draw all contour lines and LINE_AA for an anti-aliased line\n",
    "cv.drawContours(image=test_copy, contours=find_contours(test_copy), contourIdx=-1, color=(0,255,0), thickness=1, lineType=cv.LINE_AA)\n",
    "\n",
    "# Display the original image and the contours\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv.cvtColor(test_image, cv.COLOR_BGR2RGB))\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Contours Image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv.cvtColor(test_copy, cv.COLOR_BGR2RGB))\n",
    "plt.title('Contours')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c303a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List containing contour values for each image\n",
    "contours_list = []\n",
    "for image in image_list:\n",
    "    contours_list.append(find_contours(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79a3a8d",
   "metadata": {},
   "source": [
    "In the first part, the images are loaded and gathered to a list (image_list).  \n",
    "The function for finding contours is used to gather all contours to contours_list.  \n",
    "The test image (Basmati (244)) also utilizes the function and a copy is used to display the original image and it's contours  \n",
    "Main source: https://learnopencv.com/contour-detection-using-opencv-python-c/#Drawing-Contours-using-CHAIN_APPROX_NONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8386fd1",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b9d9c",
   "metadata": {},
   "source": [
    "Color features extraction. RGB images are converted to YCbCr format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db75fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycrcb_list = []\n",
    "# Converting the images to YCrCb color space and appending them to the list\n",
    "for image in image_list:\n",
    "    ycrcb_list.append(cv.cvtColor(image, cv.COLOR_BGR2YCrCb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883626c4",
   "metadata": {},
   "source": [
    "The images are stored in YCrCb -format to a list (OpenCV's defined ordering for the conversion). The output is in the form of \"[1,2,3]\" where:\n",
    "\n",
    "1 = Y (Luma) - The brightness of the pixel   \n",
    "2 = Cr (Chrominance red) - The red difference  \n",
    "3 = Cb (Chrominance blue) - The blue difference      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ffd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixels_in_contour(image, contour):\n",
    "    # Initialize lists to store Y, Cb, Cr values within contour\n",
    "    y_values = []\n",
    "    cr_values = []\n",
    "    cb_values = []\n",
    "    # Iterate over each pixel in the image\n",
    "    for y in range(image.shape[0]):\n",
    "        for x in range(image.shape[1]):\n",
    "            # Check if the pixel is within the contour\n",
    "            if cv.pointPolygonTest(contour, (x, y), measureDist=False) == 1:\n",
    "                # Get YCrCb values at the pixel location\n",
    "                y_value, cr_value, cb_value = image[y, x] # indexing is reversed\n",
    "                # Append Y, Cb, Cr values to lists\n",
    "                y_values.append(y_value)\n",
    "                cr_values.append(cr_value)\n",
    "                cb_values.append(cb_value)\n",
    "    return y_values, cr_values, cb_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc4c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ycrcb_values = {}\n",
    " # Iterate over each image and its contours and add Y, Cb, Cr values to the dictionary\n",
    "for indx, image in enumerate(ycrcb_list):\n",
    "    y_values, cr_values, cb_values = pixels_in_contour(image, contours_list[indx][0])\n",
    "    all_ycrcb_values[indx] = {'y_values': y_values, 'cr_values': cr_values, 'cb_values': cb_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_stats(component):\n",
    "    # Calculate the mean, variance, skewness, and kurtosis\n",
    "    mean = np.mean(component)\n",
    "    var = np.var(component)\n",
    "    skewness = skew(component)\n",
    "    kurt = kurtosis(component)\n",
    "    return mean, var, skewness, kurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b1d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_stats, cr_stats, cb_stats = [], [], []\n",
    "\n",
    "# Add all Y, Cb, Cr stats to respective lists\n",
    "for i in range(500):\n",
    "    y_stats.append(comp_stats(all_ycrcb_values[i]['y_values']))\n",
    "    cr_stats.append(comp_stats(all_ycrcb_values[i]['cr_values']))\n",
    "    cb_stats.append(comp_stats(all_ycrcb_values[i]['cb_values']))\n",
    "\n",
    "# Suppress specific runtime warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"RuntimeWarning:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a0494",
   "metadata": {},
   "source": [
    "Testing if  point x = 125, y = 160 is within the contour in the test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_contour = find_contours(test_image)[0]\n",
    "\n",
    "if cv.pointPolygonTest(test_contour, (125, 160), measureDist=False) >= 0:\n",
    "    print(\"Point (125, 160) is inside the contour\")\n",
    "else:\n",
    "    print(\"Point (125, 160) is outside the contour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7772d6d",
   "metadata": {},
   "source": [
    "Mean values of Y, Cb and Cr components for the test image (within the contour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edcffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image Y, Cb, Cr means\n",
    "test_ycrcb = cv.cvtColor(test_image, cv.COLOR_BGR2YCrCb)\n",
    "test_y_values, test_cr_values, test_cb_values = pixels_in_contour(test_ycrcb, test_contour)\n",
    "test_mean_y, test_mean_cr, test_mean_cb = np.mean(test_y_values), np.mean(test_cr_values), np.mean(test_cb_values)\n",
    "print(\"Test image Y mean: \", test_mean_y, \"\\nTest image Cr mean: \", test_mean_cr, \"\\nTest image Cb mean: \", test_mean_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efafa52",
   "metadata": {},
   "source": [
    "In the second part, the images are converted to YCrCb-format due to openCV's conversion mode.  \n",
    "These are then scanned pixel by pixel to determine their values at their corresponding contours coordinates.  \n",
    "The values are then used to calculate the mean, variance, skewness and kurtosis of each YCrCb component for the sample sets and test images.  \n",
    "Main source: https://docs.opencv.org/3.4/d8/d01/group__imgproc__color__conversions.html#ga397ae87e1288a81d2363b61574eb8cab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc994d",
   "metadata": {},
   "source": [
    "Dimension features extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for ellipse fitting\n",
    "def fit_ellipse(contour):\n",
    "    # Fit an ellipse to the contour\n",
    "    ellipse = cv.fitEllipse(contour)\n",
    "    return ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f772f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ellipses for all the images (Had to lower the threshold value in find_contours, since all images didn't have contours)\n",
    "ellipse_list = []\n",
    "for indx, image in enumerate(image_list):\n",
    "    contours = contours_list[indx]\n",
    "    cnt = contours[0] # the first contour\n",
    "    ellipse = cv.fitEllipse(cnt)\n",
    "    ellipse_list.append(ellipse)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rice species in the image_list are: 0-100 Arborio, 101-200 Basmati, 201-300 Ipsala, 301-400 Jasmine, 401-500 Karacadag \n",
    "# We only need to extract the contours for the first image of each species\n",
    "fig, axs = plt.subplots(1, 5, figsize=(15, 4))\n",
    "for idx, i in enumerate([0, 100, 200, 300, 400]):\n",
    "    image_copy = image_list[i].copy()\n",
    "    cv.ellipse(image_copy, ellipse_list[i], (0,255,0), 2)\n",
    "    \n",
    "    # Species name fetched from the directory name in subset list\n",
    "    axs[idx].imshow(image_copy) \n",
    "    axs[idx].set_title(os.path.basename(os.path.dirname(subset[i])))\n",
    "    axs[idx].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34416988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension features\n",
    "MajorAxisLength = []\n",
    "MinorAxisLength = []\n",
    "AreaContour = []\n",
    "PerimeterContour = []\n",
    "EquivalentDiameter = []\n",
    "Compactness = []\n",
    "Shape_Factor_1 = []\n",
    "Shape_Factor_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d766c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the features for all of the images \n",
    "for indx, ellipse in enumerate(ellipse_list):\n",
    "    L = max(ellipse[1]); MajorAxisLength.append(L)\n",
    "    l = min(ellipse[1]); MinorAxisLength.append(l)\n",
    "    A = cv.contourArea(contours_list[indx][0]); AreaContour.append(A)\n",
    "    P = cv.arcLength(contours_list[indx][0], True); PerimeterContour.append(P)\n",
    "    ED = np.sqrt((4*A) / (np.pi)); EquivalentDiameter.append(ED)\n",
    "    Co = ED/L; Compactness.append(Co)\n",
    "    SF1 = L/A; Shape_Factor_1.append(SF1)\n",
    "    SF2 = l/A; Shape_Factor_2.append(SF2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b4f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image dimension features\n",
    "test_ellipse = fit_ellipse(test_contour)\n",
    "print(\"Test image features:\")\n",
    "print(\"Major axis length: \", max(test_ellipse[1]))\n",
    "print(\"Minor axis length: \", min(test_ellipse[1]))\n",
    "print(\"Contour area: \", cv.contourArea(test_contour))\n",
    "print(\"Contour perimeter: \", cv.arcLength(test_contour, True))\n",
    "test_ED = np.sqrt((4*A) / (np.pi)) # Used later for comparison\n",
    "print(\"Equivalent diameter: \", test_ED)\n",
    "print(\"Compactness: \", np.sqrt((4*cv.contourArea(test_contour)/np.pi))/max(test_ellipse[1]))\n",
    "print(\"Shape factor 1: \", max(test_ellipse[1])/cv.contourArea(test_contour))\n",
    "print(\"Shape factor 2: \", min(test_ellipse[1])/cv.contourArea(test_contour))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234acc62",
   "metadata": {},
   "source": [
    "The third part included ellipse fitting, and plotting of each species. \n",
    "The ellipses were collected to a list which was later used to calculate the features. \n",
    "The test images dimensions are also calculated:\n",
    "Main sources: Original article and    \n",
    " https://docs.opencv.org/3.4/d6/d6e/group__imgproc__draw.html#ga28b2267d35786f5f890ca167236cbc69"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316fdf5e",
   "metadata": {},
   "source": [
    "\n",
    "All features were gathered into a dataframe, with each row representing one sample and its associated feature values. Information about the original image and the rice species label were also included for each data point. This data was saved as a parquet file in the 'training_data' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd657a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For image label index\n",
    "species_index =[]\n",
    "for i in range(5):\n",
    "    for j in range(100):\n",
    "         species_index.append(i+1)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a927034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting the features into a dataframe\n",
    "featuresDF = pd.DataFrame({\n",
    "    'Label': [os.path.basename(os.path.dirname(species)) for species in subset],\n",
    "    'Original_Image_Path': subset,\n",
    "    'ImageIndex' : species_index,\n",
    "    'MajorAxisLength': MajorAxisLength, \n",
    "    'MinorAxisLength': MinorAxisLength, \n",
    "    'AreaContour': AreaContour, \n",
    "    'PerimeterContour': PerimeterContour, \n",
    "    'EquivalentDiameter': EquivalentDiameter, \n",
    "    'Compactness': Compactness, \n",
    "    'Shape_Factor_1': Shape_Factor_1, \n",
    "    'Shape_Factor_2': Shape_Factor_2\n",
    "    })\n",
    "# Adding the Y, Cr, Cb statistics to the dataframe\n",
    "featuresDF['Y_mean'] = [y[0] for y in y_stats]\n",
    "featuresDF['Y_var'] = [y[1] for y in y_stats]\n",
    "featuresDF['Y_skew'] = [y[2] for y in y_stats]\n",
    "featuresDF['Y_kurt'] = [y[3] for y in y_stats]\n",
    "featuresDF['Cr_mean'] = [cr[0] for cr in cr_stats]\n",
    "featuresDF['Cr_var'] = [cr[1] for cr in cr_stats]\n",
    "featuresDF['Cr_skew'] = [cr[2] for cr in cr_stats]\n",
    "featuresDF['Cr_kurt'] = [cr[3] for cr in cr_stats]\n",
    "featuresDF['Cb_mean'] = [cb[0] for cb in cb_stats]\n",
    "featuresDF['Cb_var'] = [cb[1] for cb in cb_stats]\n",
    "featuresDF['Cb_skew'] = [cb[2] for cb in cb_stats]\n",
    "featuresDF['Cb_kurt'] = [cb[3] for cb in cb_stats]\n",
    "\n",
    "# Saving the dataframe to /training_data as a parquet file\n",
    "featuresDF.to_parquet('../training_data/Features.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280b02f",
   "metadata": {},
   "source": [
    "Maximum variance of the Cr component for each rice species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a2a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresDF.groupby('Label').Cr_var.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987a43c8",
   "metadata": {},
   "source": [
    "Minimum equivalent diameter for each rice species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ff88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresDF.groupby('Label').EquivalentDiameter.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93a3e8",
   "metadata": {},
   "source": [
    "Minimum, maximum and median equivalent diameter for Basmati rice samples. <br>\n",
    "This is compared to the equivalent diameter value of the test_image. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basmati equivalent diameter statistics compared to the test image\n",
    "featuresDF_basmati = featuresDF.iloc[100:200]\n",
    "print(featuresDF_basmati.EquivalentDiameter.describe())\n",
    "print('Test image equivalent diameter: ', test_ED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ace861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting min and max compactness images\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_list[featuresDF[featuresDF.Compactness == featuresDF.Compactness.max()].index[0]])\n",
    "plt.subplot(1, 2, 2)    \n",
    "plt.imshow(image_list[featuresDF[featuresDF.Compactness == featuresDF.Compactness.min()].index[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232d92b",
   "metadata": {},
   "source": [
    "\n",
    "In order to assess the effect of the compactness value on the shape of the rice, we can illustrate it by plotting the images with the maximum and minimum compactness values, as presented by the code above.  \n",
    "The pictures show that the higher the compactness value, the rounder the rice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c83db",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The last part included gathering all of the features in to a dataframe (featureDF).  \n",
    "For each species, these features were used to determine maximum variance of the Cr component and minimum equivalent diameter. The minimum, maximum and median ED was also calculated for the Basmati samples.\n",
    "Lastly, the compactness values affect was evaluated.  \n",
    "Main source: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb7a35a",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee7742",
   "metadata": {},
   "source": [
    "Import training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../training_data/Features.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eadc159",
   "metadata": {},
   "source": [
    "Standardization using z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1812b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numeric data and standardize\n",
    "scaler = StandardScaler()\n",
    "num_data = df.drop(df.columns[[0, 1, 2]], axis=1)\n",
    "num_data_z = pd.DataFrame(scaler.fit_transform(num_data), columns=num_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c65d6",
   "metadata": {},
   "source": [
    "Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61073e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "labels = num_data_z.columns\n",
    "\n",
    "#Species\n",
    "species = df['Label'].unique()\n",
    "\n",
    "# Colors\n",
    "colors = ['blue', 'green', 'red', 'cyan', 'magenta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 5\n",
    "num_cols = 4\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(30, 30))\n",
    "\n",
    "# Flatten the axes to iterate over them\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over each feature and plot it\n",
    "for i, feature in enumerate(num_data_z.columns):\n",
    "    for j in range(5):  # Iterate over each segment of the data\n",
    "        ax = axes[i]\n",
    "        ax.hist(num_data_z.iloc[j * 100: (j + 1) * 100][feature], color=colors[j], label=species[j], edgecolor='black', bins=15)\n",
    "        ax.set_title(feature)\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Nan values to 0\n",
    "num_data_z.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e0adb",
   "metadata": {},
   "source": [
    "Pairplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add image index to copy for hue\n",
    "num_data_z_ind = num_data_z.copy()\n",
    "num_data_z_ind['ImageIndex'] = df['ImageIndex']\n",
    "\n",
    "# Pairplot\n",
    "sns.pairplot(num_data_z_ind, hue='ImageIndex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bb13b9",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374dae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(num_data_z)\n",
    "\n",
    "pca_data_w_labels = pd.DataFrame(data=pca_data, columns=['PC1','PC2'])\n",
    "pca_data_w_labels['species'] = df['Label']\n",
    "\n",
    "# Plot PCA with scatterplot\n",
    "sns.scatterplot(x=pca_data_w_labels['PC1'], y=pca_data_w_labels['PC2'], hue=pca_data_w_labels['species'])\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(cumulative_variance_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03140ad2",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d5fdf",
   "metadata": {},
   "source": [
    "Features with discriminative power:\n",
    "\n",
    "The compactness plot’s results show that for each species, the values mainly do not overlap. This would suggest that the compactness feature has discriminative and predictive power regarding the target variable. Shape Factor 1&2 and MajorAxisLength also has some discriminative power as the bins do not fully overlap and spreads evenly across the z-score range, but compactness feature has the clearest separation.\n",
    "\n",
    "Correlations:\n",
    "\n",
    "According to the pairplots, equivalent diameter and contour area features has the highest correlance, as the plots have clear upwards-sloping lines with minimal spread. shape factor’s 1 & 2 negatively correlate to the major- and minor axis lengths, meaning that as the shape factor values rise, the major and minor axis length values decrease and vice versa.\n",
    "\n",
    "Clusters:\n",
    "\n",
    "According to the PCA figure, ipsala has a clear cluster. Basmati has some points that fall under jasmine’s outliers but overall, they are distinguishable. Karacadag’s and arborio’s values are close to each other but still distinguishable and karacadag values have minimal spread. Overall, as the data points are well spread out and distinguishable, the PCA reduction appears to have successfully captured the structure of the data and the classification of the rice species should be\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c19a97d",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X = num_data_z\n",
    "y = df['ImageIndex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(param, clf):\n",
    "  kf = RepeatedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "  # GridSearch CV\n",
    "  grid = GridSearchCV(clf, param, cv=kf)\n",
    "  grid.fit(X, y)\n",
    "\n",
    "  # Report the selected combination of hyperparameters\n",
    "  print(\"Best Parameters:\", grid.best_params_)\n",
    "\n",
    "  # Retrieve the cross-validated accuracy for each hyperparameter combination\n",
    "  cv_accuracy = grid.cv_results_['mean_test_score']\n",
    "\n",
    "  # Print the accuracy for each hyperparameter combination\n",
    "  for idx, accuracy in enumerate(cv_accuracy):\n",
    "      print('Accuracy:', accuracy, grid.cv_results_['params'][idx])\n",
    "      \n",
    "  warnings.filterwarnings(\"ignore\", message=\".*ConvergenceWarning:.*\")\n",
    "  return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68219b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "rfparam = {\n",
    "    'n_estimators': np.arange(100, 301, 50),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "# Classifier\n",
    "rfc = RandomForestClassifier(random_state=1)\n",
    "\n",
    "best_est = classify(rfparam, rfc)\n",
    "\n",
    "# Fit the best model to retrieve feature importance\n",
    "best_model = best_est\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Feature Importance\n",
    "print(\"Feature Importance:\")\n",
    "feature_importance = best_model.feature_importances_\n",
    "for i, importance in enumerate(feature_importance):\n",
    "    print(f\"Feature {i+1}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svmparam = {\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'C' : [0.1, 1, 10, 100],\n",
    "    'kernel' : ['linear', 'rbf', 'poly']\n",
    "}\n",
    "# Classifier\n",
    "svmf = svm.SVC(random_state=1)\n",
    "\n",
    "classify(svmparam, svmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a9791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "mlpparam = {\n",
    "  'hidden_layer_sizes': [(n,) for n in range(15, 41, 5)],  # Number of neurons in the hidden layer\n",
    "  'activation': ['tanh', 'relu'],  # Activation functions\n",
    "  'solver': ['sgd', 'adam'],  # Solvers\n",
    "  'alpha': [0.01, 0.1, 1],  # L2 regularization strength\n",
    "  'validation_fraction': [0.1, 0.3]  # Validation fraction\n",
    "}\n",
    "# Classifier\n",
    "mlpc = MLPClassifier(random_state=1)\n",
    "classify(mlpparam, mlpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268e06b5",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f991b",
   "metadata": {},
   "source": [
    "For model selection, accuracy was used to determine performance which only captions the correct predictions to all predictions. Apart from cross validating and determining the best hyperparameters based on accuracy, other evaluation methods can be used such as F1-score, which combines the precision and recall scores, which measure the proportion of true positives to all positives and all actual positive instances. These would more efficiently capture the models performance. Area under ROC Curve (AUC) can also be plotted and utilized to estimate performance. \n",
    "\n",
    "The model selection process can also be done with different cross validation techniques, such as stratified k-fold, which takes into account that each fold has the same proportion of classes and therefore is efficient in imbalanced datasets where certain features may dominate the set. Leave-one-out cross validation can also be considered as it doesn’t have repeated iterations and the used repeated k-fold technique was a computationally heavy process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860a4c7",
   "metadata": {},
   "source": [
    "## Performance estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e515b",
   "metadata": {},
   "source": [
    "Performance estimation using 5-fold repeated cross validation with 3 repetitions and the same parameter ranges as in Part 2 for the inner loop. The random forest tree nested cross-validation was provided beforehand as its computationally very heavy.  <br>\n",
    "\n",
    "For the outer loop we'll use 10-fold Stratified Kfold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4801680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Tree hyperparameter tuning and training results\n",
    "results = pd.read_parquet('../training_data/rf_results.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b314a",
   "metadata": {},
   "source": [
    "Distribution of the selected parameter combinations and mean accuracy and confusion matrix calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.best_params.value_counts())\n",
    "\n",
    "fold_accuracy = results.groupby('fold')['accuracy'].mean()\n",
    "full_mean = results.accuracy.mean()\n",
    "print(fold_accuracy)\n",
    "print(full_mean)\n",
    "\n",
    "true_class = []\n",
    "\n",
    "for acc, pred in zip(results.accuracy, results.pred_class):\n",
    "    if acc == 1:\n",
    "        true_class.append(pred)\n",
    "    else:\n",
    "        true_class.append(\"else\")\n",
    "\n",
    "print(set(true_class))\n",
    "conf_matrix = metrics.confusion_matrix(true_class, results.pred_class, labels=['Kar', 'Arb', 'Ips', 'Jas', 'Bas'])\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb5bbe",
   "metadata": {},
   "source": [
    " Nested cross-validation using *cross_val_predict* function from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_Z = ['mean_y_Z', 'var_y_Z',\n",
    " 'skew_y_Z', 'kurt_y_Z',\n",
    " 'mean_cr_Z', 'var_cr_Z',\n",
    " 'skew_cr_Z', 'kurt_cr_Z',\n",
    " 'mean_cb_Z', 'var_cb_Z',\n",
    " 'skew_cb_Z', 'kurt_cb_Z',\n",
    " 'major_axis_length_Z',\n",
    " 'minor_axis_length_Z',\n",
    " 'area_Z', 'perimeter_Z',\n",
    " 'equivalent_diameter_Z',\n",
    " 'compactness_Z',\n",
    " 'shape_factor1_Z',\n",
    " 'shape_factor2_Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b9aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = results['class']\n",
    "X = results[feats_Z]\n",
    "\n",
    "rf = RandomForestClassifier(random_state=20)\n",
    "mlp = MLPClassifier(random_state=20)\n",
    "svm = SVC(random_state=20)\n",
    "\n",
    "kf_outer = StratifiedKFold(n_splits=10, random_state=10, shuffle=True)\n",
    "kf_inner = RepeatedKFold(n_splits=5, n_repeats=3, random_state=5)\n",
    "\n",
    "n_estimators = range(100, 350, 50)\n",
    "max_features = ['sqrt', 'log2', None]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "rf_parameters={\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'bootstrap': bootstrap}\n",
    "\n",
    "mlp_parameters = {\n",
    "    'hidden_layer_sizes': [(n,) for n in range(15, 41, 5)],\n",
    "  'activation': ['tanh', 'relu'],\n",
    "  'solver': ['sgd', 'adam'],\n",
    "  'alpha': [0.01, 0.1, 1],\n",
    "  'validation_fraction': [0.1, 0.3]\n",
    "}\n",
    "svm_parameters = {\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'C' : [0.1, 1, 10, 100],\n",
    "    'kernel' : ['linear', 'rbf', 'poly']\n",
    "}\n",
    "gscv_rf = GridSearchCV(rf, rf_parameters, cv=kf_inner, return_train_score=False, n_jobs=-1)\n",
    "gscv_mlp = GridSearchCV(mlp, mlp_parameters, cv=kf_inner, return_train_score=False, n_jobs=-1)\n",
    "gscv_svm = GridSearchCV(svm, svm_parameters, cv=kf_inner, return_train_score=False, n_jobs=-1)\n",
    "\n",
    "# Nested cross-validation and predictions\n",
    "nested_pred_rf = cross_val_predict(gscv_rf, X, y, cv=kf_outer)\n",
    "nested_pred_mlp = cross_val_predict(gscv_mlp, X, y, cv=kf_outer)\n",
    "nested_pred_svm = cross_val_predict(gscv_svm, X, y, cv=kf_outer)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_rf = metrics.accuracy_score(y, nested_pred_rf)\n",
    "accuracy_mlp = metrics.accuracy_score(y, nested_pred_mlp)\n",
    "accuracy_svm = metrics.accuracy_score(y, nested_pred_svm)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix_rf = metrics.confusion_matrix(y, nested_pred_rf)\n",
    "conf_matrix_mlp = metrics.confusion_matrix(y, nested_pred_mlp)\n",
    "conf_matrix_svm = metrics.confusion_matrix(y, nested_pred_svm)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
    "print(\"Random Forest Confusion Matrix:\")\n",
    "print(conf_matrix_rf)\n",
    "\n",
    "print(\"\\nMLP Accuracy:\", accuracy_mlp)\n",
    "print(\"MLP Confusion Matrix:\")\n",
    "print(conf_matrix_mlp)\n",
    "\n",
    "print(\"\\nSVM Accuracy:\", accuracy_svm)\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "print(conf_matrix_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db538af",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841eaa3f",
   "metadata": {},
   "source": [
    "The results of the original article differed quite a bit from our results, as the highest accuracy score was reported for Multi Layer Perceptron at 0.9991, while in our case MLP had a score of 0.988. Support Vector Machine for all of the features had similar accuracy score (0.998) and Random Forest had a lower score in our project (0.994 vs. 0.998). The closeness of the results is still surprising, as the original data set was 75,000, whereas our project had only 500 samples and far fewer features than the original study. The  model could be useful in rice plantations but needs to be exposed to more species, samples and different imaging conditions.  \n",
    "\n",
    "Throughout the project, the small sample size had led me to mistakenly believe that prediction accuracy would be poor. With the final accuracies and the nested cross-validation with stratified and repeated k-folds, the most unbiased hyperparameters could be achieved for the best accuracy with even a small dataset, and I was surprised with the precision.   \n",
    "\n",
    "Throughout the project, I learned to inspect, craft and review my code before running it as throughout the project the runtime for the code cells exponentially grew. This was a learning experience, as I noticed that in the beginning, I brute-forced multiple commands as their correctness could be easily checked but later on this was an exhaustive method as the runtimes grew up to 30 minutes per cell.  \n",
    "\n",
    "I believe, that the visualization parts succeeded well as I aimed at making them as informational as possible so that I could also understand the data as well as possible. The methods and data processing techniques used during the project will certainly be useful in the future.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "232px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
